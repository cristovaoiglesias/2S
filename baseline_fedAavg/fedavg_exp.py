# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from scipy.stats import norm, uniform
from numpy.random import normal
import itertools

global X
global y

def f(x):
    return np.cos(x) + np.random.normal(0, 0.1, size=x.shape)

x_train = np.arange(-4, 8, 0.02)
# x_train = np.arange(-4.75, 1.7, 0.02)
y_train = np.array([f(x) for x in x_train])


# Use the x and y defined above
X = x_train
y = np.array(y_train)


# Visualize the data

# Define the function f(x) as specified

indices = np.concatenate([np.arange(1, 51), np.arange(120, 181), np.arange(280, 324), np.arange(400, 451), np.arange(540, 571)])
X = np.tile(x_train[indices],15).reshape(-1, 1)
y = y_train[indices]

y = np.array(
    [-0.6546246153513355, -0.6770799364919654, -0.7324382523078716, -0.7275967209243985, -0.8627970578669415, -0.6485857174293695, -0.8444840852285688, -0.6356793765260966, -0.9844408736920812, -0.8147252368220123, -0.7960903017286002, -0.9282439614393723, -0.7881530572792456, -0.7713350455751763, -0.8459597802933861, -0.8293980230988941, -0.897507892419915, -0.7821771400592701, -0.9149096347993144, -0.804437755918612, -1.0934923821801346, -0.9664845790300806, -0.752701330074209, -0.9999360889403396, -0.9148039799693455, -1.1109600752244937, -1.0398290070543879, -0.7730582810954396, -1.0790714544055717, -0.9244675628540262, -1.0560584574947216, -0.9449109368447617, -0.9201055131041548, -0.8764418328711275, -0.9666476926602, -0.9104128208045433, -1.1494350666351933, -0.9670439419280831, -0.9696354198012063, -1.0905488957145872, -1.022975547180027, -1.0582131554290137, -1.1103322783337057, -0.9408110672856214, -0.9416275915412365, -0.9777370489914523, -0.9684093233651259, -0.9329837090760447, -1.0040169336235762, -0.9344297655525666, 0.060954848078509684, -0.043313990107577686, 0.04911638958814058, 0.13892072632292832, 0.10694610031485081, 0.10276106207112469, -0.033343034770978885, -0.17084674923821988, 0.08362128608906143, 0.2307944291356918, 0.09312682582652898, 0.2648212873799939, 0.18398604900443605, 0.23837766936596924, 0.20237070291921982, 0.15350381026166737, 0.2808660090297438, 0.4070354922066405, 0.20261254282391522, 0.4992186464297875, 0.3085856373013179, 0.1866474801835296, 0.320990843648573, 0.4453808440242035, 0.5043707383173087, 0.44120126135335147, 0.47082626700161134, 0.4914152198708356, 0.5337674646818206, 0.6292470704440416, 0.5604192213270424, 0.509941687930124, 0.49893016649452127, 0.5255648046132274, 0.6259112484997438, 0.6823750761722212, 0.6964831170289916, 0.7609017716386202, 0.8452402367073831, 0.6573796568626006, 0.515408242168034, 0.5871753662212371, 0.7118015256790903, 0.6855074348059969, 0.5948363771857784, 0.7998306603280494, 0.7209940472207097, 0.9850264196183349, 0.7228572763565881, 0.7867116155009874, 0.9717646749267623, 0.8597917600170926, 0.8330576167767547, 0.8495930196701957, 0.8789668973003498, 1.0385753093019092, 0.9655611773413234, 1.0129036950050323, 0.8402929698053146, 0.750065490019575, 0.7681087618452245, -0.1672875585000863, -0.017268305316421165, -0.16898039826456998, -0.14679017389836344, -0.09161800819350467, 0.08089718100591939, -0.2304438199653761, -0.12915765262422174, -0.08331020897104509, -0.1789589337150457, -0.44715635535437753, -0.2977347368517, -0.2749195671103246, -0.18049048297294498, -0.3785779849092903, -0.09684818107657905, -0.3341803449752112, -0.46257031194128095, -0.4639870852532688, -0.3721337405278696, -0.4523311983015491, -0.33933041982835466, -0.44204435952028576, -0.4845108566487512, -0.48656540308369134, -0.5446537716543984, -0.4148819218419479, -0.3981202265924692, -0.6174271176235989, -0.6137621105938429, -0.5542473277600418, -0.5332227593850133, -0.49632326133483046, -0.705841163260712, -0.6342371297806348, -0.8133238270644332, -0.6638112099556129, -0.7226798559510678, -0.7009322784112065, -0.6680974664278189, -0.6265491696102692, -0.7973901850249783, -0.8744769937914745, -0.939913009303357, -0.7422237672745747, -0.7154994584408676, -0.8141867081575336, -0.48292137250107486, -0.6081460058502854, -0.5919606227139629, -0.6557831607638429, -0.6465854509984261, -0.478105064983153, -0.6094817300999614, -0.46614925542741154, -0.49167738332665945, -0.3302035178584979, -0.4998535825358329, -0.4879476897838255, -0.5512389599741483, -0.3004660222014187, -0.2853366818423542, -0.33598224515147207, -0.38926206298992444, -0.3657458529361364, -0.46144486054567113, -0.2274226360128913, -0.2504872251764845, -0.02361572349224597, -0.13567749688548492, -0.3226025183130214, -0.19000245499904864, 0.03593956125485803, -0.09519506408124825, -0.2112309317613728, -0.1582278578431451, 0.018701576838815917, -0.08872289864413006, -0.13919092519198378, -0.03748543010870714, 0.06721408476825749, 0.01864725423452032, 0.03970295653150732, 0.057579732396309605, -0.11490236262922533, 0.09793336598237004, 0.0973177322634333, 0.054846728878361325, 0.14497689256151639, 0.127050102544834, 0.30462025839539086, 0.1880119809637499, 0.14433652625277585, 0.13523822268505561, 0.26053439255738203, 0.9081047715497008, 0.9355998510213308, 0.7881563308782414, 0.9085078314040601, 0.6124475792256925, 0.6862109229665616, 0.923265547538979, 0.8107454035559275, 0.8178685338355274, 0.8809773400559885, 0.6877006479942385, 0.7816928657056375, 0.9159814385101913, 0.8348817899543718, 0.5766578868710525, 0.65126383897542, 0.7130258886277903, 0.57613233783057, 0.9466532629118555, 0.7671730536501625, 0.6861260802157539, 0.6868267828925444, 0.5954221629536156, 0.47519098328994863, 0.6029052329845926, 0.6896587204580127, 0.5558760642413504, 0.445306626580954, 0.4453772015118261, 0.5183674773668522, 0.6088969975602531])

X = X
y = np.tile(y,15)

print(X.shape,y.shape)
plt.scatter(X, y, alpha=0.5)
plt.title('Scatter plot of provided data')
plt.xlabel('Feature X')
plt.ylabel('Label y')
plt.savefig('./scatter_fedavg.png')

# Function to generate IID data for a client
def generate_client_data():
    d = uniform(-5, 14)  # Uniform(-5, 9) in Julia corresponds to uniform distribution over (-5, 9) range
    x_client = d.rvs(50)
    y_client = f(x_client)
    return x_client.reshape(-1, 1), y_client

# Define the Neural Network Model in PyTorch
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(1, 20)
        self.fc2 = nn.Linear(20, 20)
        self.fc3 = nn.Linear(20, 1)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

def RMSPE(ground_truth, estimation):
    mspe = np.mean(((ground_truth-estimation)/ground_truth) ** 2)
    rmspe = np.sqrt(mspe)
    return rmspe *100

# Function to train a local model with initial global model parameters
def train_local_model(X_train, y_train, global_model, num_epochs=350, learning_rate=0.001):
    model = NeuralNetwork()
    model.load_state_dict(global_model.state_dict())  # Load the global model's parameters

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Convert data to PyTorch tensors
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)

    # Training loop
    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train_tensor)
        loss = criterion(outputs, y_train_tensor)
        loss.backward()
        optimizer.step()

    return model

# Function to average the weights of local models to create a global model
def average_models(models, rr):
    avg_model = NeuralNetwork()
    model_dicts = [model.state_dict() for model in models]
    avg_dict = model_dicts[0]

    for key in avg_dict.keys():
        for model_dict in model_dicts[1:]:
            avg_dict[key] += model_dict[key]
        avg_dict[key] = torch.div(avg_dict[key], len(models))

    avg_model.load_state_dict(avg_dict)

    # Plotting the true function vs predictions
    x_plot = np.arange(-5, 9, 0.02).reshape(-1, 1)  # x range for true function and predictions
    y_true = np.cos(x_plot)  # true values using the function f

    with torch.no_grad():
        y_pred_tensor = avg_model(torch.tensor(x_plot, dtype=torch.float32))
        y_pred = y_pred_tensor.numpy()  # predictions using the global model

    plt.figure(figsize=(10, 6))
    plt.plot(x_plot, y_true, label='True function (f(x))', color='blue')
    plt.plot(x_plot, y_pred, label='Global Model Predictions', color='red', linestyle='--')

    # Plot the initial data points (from the first client in the first round)
    initial_data_x, initial_data_y = X, y
    plt.scatter(initial_data_x, initial_data_y, color='green', label='Initial Data Points', alpha=0.5)
    print(f"FedAVG (server) round{rr} - RMSPE(true,pred FedAVG): ", RMSPE(np.array(list(map(np.cos, x_plot)))+5, y_pred+5))
    plt.title(f'True Function vs Global Model Predictions - Round {rr}')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.savefig(f'./result_FedAVG_{rr}.png')
    plt.close()

    return avg_model

# Function to evaluate the global model
def evaluate_model(model, X_test, y_test):
    model.eval()
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

    with torch.no_grad():
        y_pred_tensor = model(X_test_tensor)

    y_pred = y_pred_tensor.numpy()
    mse = mean_squared_error(y_test, y_pred)
    return mse, y_pred

# Initialize the global model
global_model = NeuralNetwork()

X_train_tensor_first = torch.tensor(X, dtype=torch.float32)
y_train_tensor_first = torch.tensor(y, dtype=torch.float32).view(-1, 1)
optimizer = optim.Adam(global_model.parameters(), lr=0.001)
criterion = nn.MSELoss()

for epoch in range(350):
        global_model.train()
        optimizer.zero_grad()
        outputs = global_model(X_train_tensor_first)
        loss = criterion(outputs, y_train_tensor_first)
        loss.backward()
        optimizer.step()

global_model = average_models([global_model], 0)
# Run the federated learning process over 100 communication rounds
num_clients = 100
num_rounds = 100

# Generate the initial data points for visualization before starting the rounds
client_data = [generate_client_data() for _ in range(num_clients)]

for round_num in range(num_rounds):
    print(f"Communication Round {round_num + 1}/{num_rounds}")

    # Generate data and train local models on each client
    local_models = []
    for _ in range(num_clients):
        X_client, y_client = generate_client_data()
        model = train_local_model(X_client, y_client, global_model)
        local_models.append(model)

    #for _ in range(5):
    #    d = uniform(-5, 2)
    #    x_client = d.rvs(50)
    #    y_client = f(x_client)
    #    x_client = x_client.reshape(-1, 1)
    #    model = train_local_model(x_client, y_client, global_model)
    #    local_models.append(model)

    #for _ in range(5):
    #    d = uniform(-5, 9)
    #    x_client = d.rvs(50)
    #    y_client = f(x_client) + (0.5 * np.random.normal(0, 0.1, size=x_client.shape))
    #    x_client = x_client.reshape(-1, 1)
    #    model = train_local_model(x_client, y_client, global_model)
    #    local_models.append(model)

    #for _ in range(25):
    #   d = uniform(-5, 2)
    #    x_client = d.rvs(50)
    #    y_client = f(x_client) + (0.5 * np.random.normal(0, 0.1, size=x_client.shape)) + 0.15
    #    x_client = x_client.reshape(-1, 1)
    #    model = train_local_model(x_client, y_client, global_model)
    #    local_models.append(model)

    # Aggregate local models into the global model
    global_model = average_models(local_models, round_num+1)
